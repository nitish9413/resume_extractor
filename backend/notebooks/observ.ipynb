{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2514f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://192.168.1.15:6006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42f20f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔭 OpenTelemetry Tracing Details 🔭\n",
      "|  Phoenix Project: my-llm-app\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: 192.168.1.15:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=\"my-llm-app\", # Default is 'default'\n",
    "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bc55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0deb0671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 36, 'total_tokens': 44, 'completion_time': 0.017230522, 'prompt_time': 0.001930722, 'queue_time': 0.052759558, 'total_time': 0.019161244}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_0f5c9bc037', 'finish_reason': 'stop', 'logprobs': None}, id='run--6fe458e8-f4ca-478f-94e6-a5d49cffe1c2-0', usage_metadata={'input_tokens': 36, 'output_tokens': 8, 'total_tokens': 44})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6467324f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here\\'s a step-by-step guide on how to add observability to a Polars data pipeline in Python using Prometheus, Grafana, and New Relic.\\n\\n**Step 1: Install Required Libraries**\\n\\nFirst, install the required libraries using pip:\\n\\n```bash\\npip install polars prometheus-client newrelic\\n```\\n\\n**Step 2: Collect Polars Metrics**\\n\\nTo collect Polars metrics, we\\'ll use the `polars.metrics` module. Here\\'s an example:\\n\\n```python\\nimport polars as pl\\nfrom prometheus_client import Counter, Gauge, Histogram\\n\\n# Create a counter to track the number of rows processed\\nrows_processed_counter = Counter(\\n    \\'polars_rows_processed\\',\\n    \\'Number of rows processed by Polars\\',\\n    [\\'stage\\']\\n)\\n\\n# Create a gauge to track the number of rows in the pipeline\\nrows_in_pipeline_gauge = Gauge(\\n    \\'polars_rows_in_pipeline\\',\\n    \\'Number of rows in the Polars pipeline\\',\\n    [\\'stage\\']\\n)\\n\\n# Create a histogram to track the execution time of Polars operations\\nexecution_time_histogram = Histogram(\\n    \\'polars_execution_time\\',\\n    \\'Execution time of Polars operations\\',\\n    [\\'stage\\']\\n)\\n```\\n\\n**Step 3: Instrument Polars Pipeline**\\n\\nNow, let\\'s instrument the Polars pipeline by creating a decorator that will track the metrics:\\n\\n```python\\ndef instrument_pipeline(func):\\n    def wrapper(*args, **kwargs):\\n        # Get the current stage (e.g., \"read\", \"filter\", \"join\")\\n        stage = args[0].stage\\n\\n        # Record the start time\\n        start_time = pl.timestamp()\\n\\n        # Call the original function\\n        result = func(*args, **kwargs)\\n\\n        # Record the end time\\n        end_time = pl.timestamp()\\n\\n        # Calculate the execution time\\n        execution_time = end_time - start_time\\n\\n        # Record the metrics\\n        rows_processed_counter.inc(len(result), stage=stage)\\n        rows_in_pipeline_gauge.set(len(result), stage=stage)\\n        execution_time_histogram.observe(execution_time, stage=stage)\\n\\n        return result\\n    return wrapper\\n```\\n\\n**Step 4: Apply the Decorator to Polars Operations**\\n\\nApply the `instrument_pipeline` decorator to the Polars operations in your pipeline:\\n\\n```python\\n@instrument_pipeline\\ndef read_data(df):\\n    # Read data from a file or database\\n    return pl.read_csv(\\'data.csv\\')\\n\\n@instrument_pipeline\\ndef filter_data(df):\\n    # Filter data using a predicate\\n    return df.filter(pl.col(\\'age\\') > 30)\\n\\n@instrument_pipeline\\ndef join_data(df1, df2):\\n    # Join two dataframes\\n    return df1.join(df2, on=\\'id\\')\\n```\\n\\n**Step 5: Push Metrics to Prometheus**\\n\\nTo push the metrics to Prometheus, you\\'ll need to create a Prometheus server and configure it to scrape the metrics. Here\\'s an example using the `prometheus-client` library:\\n\\n```python\\nfrom prometheus_client import start_http_server\\n\\n# Start the Prometheus server\\nstart_http_server(8000)\\n\\n# Scrape the metrics every 10 seconds\\nimport time\\nwhile True:\\n    rows_processed_counter.inc()\\n    rows_in_pipeline_gauge.set(100)\\n    execution_time_histogram.observe(0.1)\\n    time.sleep(10)\\n```\\n\\n**Step 6: Visualize Metrics with Grafana**\\n\\nTo visualize the metrics, create a Grafana dashboard and add the following panels:\\n\\n* A gauge panel to display the number of rows processed\\n* A gauge panel to display the number of rows in the pipeline\\n* A histogram panel to display the execution time of Polars operations\\n\\n**Step 7: Integrate with New Relic**\\n\\nTo integrate with New Relic, follow these steps:\\n\\n* Create a New Relic account and install the New Relic agent\\n* Configure the New Relic agent to scrape the metrics from Prometheus\\n* Create a New Relic dashboard to visualize the metrics\\n\\nHere\\'s an example code snippet that integrates with New Relic:\\n```python\\nimport newrelic.agent\\n\\n# Initialize the New Relic agent\\nnewrelic.agent.initialize(\\'newrelic.ini\\')\\n\\n# Create a New Relic transaction\\ntransaction = newrelic.agent.current_transaction()\\n\\n# Record the metrics\\ntransaction.record_metric(\\'polars_rows_processed\\', rows_processed_counter.value)\\ntransaction.record_metric(\\'polars_rows_in_pipeline\\', rows_in_pipeline_gauge.value)\\ntransaction.record_metric(\\'polars_execution_time\\', execution_time_histogram.value)\\n```\\nNote that this is a high-level overview of the process, and you may need to modify the code to fit your specific use case. Additionally, you\\'ll need to configure the Prometheus server, Grafana dashboard, and New Relic account to collect and visualize the metrics.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 981, 'prompt_tokens': 48, 'total_tokens': 1029, 'completion_time': 1.922021728, 'prompt_time': 0.0028897, 'queue_time': 0.0494367, 'total_time': 1.9249114280000001}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'finish_reason': 'stop', 'logprobs': None}, id='run--df7c1381-2d37-4a2e-bbff-6d54e117edd9-0', usage_metadata={'input_tokens': 48, 'output_tokens': 981, 'total_tokens': 1029})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"i want to add observability to a Polars data pipeline python\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
